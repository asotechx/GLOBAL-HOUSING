## **GLOBAL HOUSING**
This project entails a comprehensive data engineering pipeline designed to ingest, process, and analyze data efficiently, enabling robust insights and decision-making.
![alt Image](https://github.com/asotechx/GLOBAL-HOUSING/blob/main/Pipeline.jpg?raw=true)

## **Tech Stack & Tools
- Data Warehouse: PostgreSQL
- Database: PostgreSQL
- Orchestration: Apache Airflow
- Data Processing: Pandas
- ETL Scripts: Python
- Visualization: PowerBI

## **Pipeline Overview**
The pipeline begins by ingesting raw data from the Kaggle API, with Python used for data extraction and initial processing. Following the ETL (Extract, Transform, Load) process, the transformed data is stored in a PostgreSQL database, which serves as the centralized storage layer. The ETL workflow is orchestrated using Apache Airflow, ensuring tasks are scheduled and executed in a systematic and reliable manner. Finally, insights generated from the processed data are visualized using Power BI, delivering actionable intelligence through interactive reports and dashboards.

## **Getting Started**
This section provides step-by-step instructions to help you set up and run the project locally for development and testing purposes.

## **Installation & Setup**
- Clone the project repository to your local machine using Git:
git clone https://github.com/GLOBAL-HOUSING/GLOBAL-HOUSING.git
cd GLOBAL-HOUSING

## **Data Extraction from PostgreSQL**
The data extraction process begins by exporting raw data from the Kaggle API. Using Python, we establish a connection to the Kaggle API to retrieve the required datasets. This extraction step is orchestrated by Apache Airflow, which ensures that data is pulled consistently and reliably into the processing environment, setting the stage for the subsequent ETL (Extract, Transform, Load) operations.

## **Transformation and Loading into the Data Warehouse Schema**
Once the data is extracted, transformation routines are applied to clean, normalize, and enrich the datasets. These transformations include:

- Data Cleaning: Handling missing values, removing duplicates, and correcting inconsistent formats.

- Normalization: Standardizing numerical and categorical values to ensure consistency across datasets.

The transformed data is subsequently loaded into a structured data warehouse schema within PostgreSQL. This schema is optimized for query performance and is specifically designed to support complex analytical queries, enabling efficient reporting and data visualization.


## **PowerBI Dashboards**
Our Power BI dashboards leverage the curated datasets to deliver interactive, insightful visualizations that support data-driven decision-making.

## **Sample Visualizations**
The following visualizations showcase key components of our Power BI dashboards, each designed to highlight critical insights and trends:




